services:
  image-only: # Base service to build the image.
    image: ${IMAGE_NAME}
    platform: "linux/amd64"
    build:
      platforms:
        - "linux/amd64"
      context: .
      target: train
      args:
        BASE_IMAGE: ubuntu:22.04                # Ubuntu: https://hub.docker.com/_/ubuntu
        CURL_IMAGE: curlimages/curl:8.00.1      # https://hub.docker.com/r/curlimages/curl/tags
        GIT_IMAGE: alpine/git:edge-2.38.1       # https://hub.docker.com/r/alpine/git/tags
        CONDA_URL:
          https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh
        # https://github.com/conda-forge/miniforge/releases/
        CONDA_INSTALL_PATH: /opt/conda      # Should be the same between stages not to brake linking.
        # https://towardsdatascience.com/conda-essential-concepts-and-tricks-e478ed53b5b#bb7b
        PROJECT_NAME: ${PROJECT_NAME}
        PACKAGE_NAME: ${PACKAGE_NAME}
        GID: ${GID}
        UID: ${UID}
        GRP: ${GRP}
        USR: ${USR}
        PASSWD: ${PASSWD}
        PROJECT_ROOT: ${PROJECT_ROOT}
        PROJECT_DIR: ${PROJECT_DIR}
        DATA_DIR: ${DATA_DIR}
        OUTPUTS_DIR: ${OUTPUTS_DIR}

  local-cpu: # Service to run the image locally with CPU only.
    extends:
      service: image-only
    tty: true
    stdin_open: true
    volumes:
      - ${LOCAL_PROJECT_DIR}:${PROJECT_DIR}
      - ${LOCAL_DATA_DIR}:${DATA_DIR}
      - ${LOCAL_OUTPUTS_DIR}:${OUTPUTS_DIR}
    environment:
      WANDB_API_KEY: ${WANDB_API_KEY}

  local-gpu: # Service to run the image locally with NVIDIA GPU.
    extends:
      service: local-cpu
    environment:
      NVIDIA_VISIBLE_DEVICES: all # Can be overridden by the user.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
